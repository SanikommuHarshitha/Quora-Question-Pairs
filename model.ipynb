{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Long Short-Term Memory\n",
    "\n",
    "**Long Short-Term Memory can learn to keep only relevant information to make predictions, and forget non relevant data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "WNL = WordNetLemmatizer()\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MIN_WORD_OCCURRENCE = 100\n",
    "REPLACE_WORD = \"memento\"\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_FOLDS = 10\n",
    "BATCH_SIZE = 1025\n",
    "EMBEDDING_FILE = \"glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatize:  Lemmatization is a Text Normalization (or sometimes called Word Normalization) technique in the field of Natural Language Processing that is used to prepare text, words, and documents for further processing.**\n",
    "**WordNetLemmetizer(WNL) is an large, freely and publicly available lexical database for the English language aiming to establish structured semantic relationships between words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutter(word):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions are preprocessed such that the different forms of writing the same thing are tried to be unified. So, LSTM does not learn different things from these different interpretations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string):\n",
    "    string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "    string = ' '.join([cutter(w) for w in string.split()])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **How many of the rare words are common in the both pairs and how many of them are numeric are used as features. This whole process leads to better generalization in LSTM so that it cannot overfit particular pairs by just using these rare words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open(EMBEDDING_FILE,encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in top_words:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding out how many of them are numeric and  used as features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The rest is considered as rare words and replaced by the word \"memento\", since \"memento\" is irrelevant to almost anything, it is absically a placeholder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(q):\n",
    "    new_q = []\n",
    "    surplus_q = []\n",
    "    numbers_q = []\n",
    "    new_memento = True\n",
    "    for w in q.split()[::-1]:\n",
    "        if w in top_words:\n",
    "            new_q = [w] + new_q\n",
    "            new_memento = True\n",
    "        elif w not in STOP_WORDS:\n",
    "            if new_memento:\n",
    "                new_q = [\"memento\"] + new_q\n",
    "                new_memento = False\n",
    "            if is_numeric(w):\n",
    "                numbers_q = [w] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [w] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    new_q = \" \".join(new_q)\n",
    "    return new_q, set(surplus_q), set(numbers_q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    q1s = np.array([\"\"] * len(df), dtype=object)\n",
    "    q2s = np.array([\"\"] * len(df), dtype=object)\n",
    "    features = np.zeros((len(df), 4))\n",
    "\n",
    "    for i, (q1, q2) in enumerate(list(zip(df[\"question1\"], df[\"question2\"]))):\n",
    "        q1s[i], surplus1, numbers1 = prepare(q1)\n",
    "        q2s[i], surplus2, numbers2 = prepare(q2)\n",
    "        features[i, 0] = len(surplus1.intersection(surplus2))\n",
    "        features[i, 1] = len(surplus1.union(surplus2))\n",
    "        features[i, 2] = len(numbers1.intersection(numbers2))\n",
    "        features[i, 3] = len(numbers1.union(numbers2))\n",
    "\n",
    "    return q1s, q2s, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data into the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train[\"question1\"] = train[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "train[\"question2\"] = train[\"question2\"].fillna(\"\").apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply preprocess() to the null and NAN values in the train and test datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "all_questions = pd.Series(train[\"question1\"].tolist() + train[\"question2\"].tolist()).unique()\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(all_questions)\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(REPLACE_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding out the words which are in top words and not in embeddings_index.keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words are not found in the embedding: {'iisc', 'kvpy', 'redmi', 'c#', '\\\\frac', 'demonetisation', 'oneplus', 'paytm', '\\\\sqrt', 'brexit', 'quorans'}\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = get_embedding()\n",
    "print(\"Words are not found in the embedding:\", top_words - embeddings_index.keys())\n",
    "top_words = embeddings_index.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the train dataset for LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train questions are being prepared for LSTM...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train questions are being prepared for LSTM...\")\n",
    "q1s_train, q2s_train, train_q_features = extract_features(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(q1s_train, q2s_train))\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pad_sequences(tokenizer.texts_to_sequences(q1s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(tokenizer.texts_to_sequences(q2s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(train[\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The features mentioned above are merged with NLP and non-NLP features. As a result, 4+15+6=25 features are prepared for the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features are being merged with NLP and Non-NLP features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train features are being merged with NLP and Non-NLP features...\")\n",
    "train_nlp_features = pd.read_csv(\"nlp_features_train.csv\")\n",
    "train_non_nlp_features = pd.read_csv(\"non_nlp_features_train.csv\")\n",
    "features_train = np.hstack((train_q_features, train_nlp_features, train_non_nlp_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same steps are being applied for test...\n"
     ]
    }
   ],
   "source": [
    "print(\"Same steps are being applied for test...\")\n",
    "test[\"question1\"] = test[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "test[\"question2\"] = test[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "q1s_test, q2s_test, test_q_features = extract_features(test)\n",
    "test_data_1 = pad_sequences(tokenizer.texts_to_sequences(q1s_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(tokenizer.texts_to_sequences(q2s_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_nlp_features = pd.read_csv(\"nlp_features_test.csv\")\n",
    "test_non_nlp_features = pd.read_csv(\"non_nlp_features_test.csv\")\n",
    "features_test = np.hstack((test_q_features, test_nlp_features, test_non_nlp_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LSTM***\n",
    "\n",
    "**The train data is divided into 10 folds. In every run, one fold is kept as the validation set for early stopping. So, every run uses 1 fold different than the other for training which can contribute to the model variance. Since we are going to ensemble the models, increasing model variance reasonably is something we may want.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "model_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 0\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3.1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3.1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3.1\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 363860 samples, validate on 40430 samples\n",
      "Epoch 1/15\n",
      "363860/363860 [==============================] - 320s 880us/step - loss: 0.2806 - val_loss: 0.2469\n",
      "Epoch 2/15\n",
      "363860/363860 [==============================] - 310s 851us/step - loss: 0.2421 - val_loss: 0.2285\n",
      "Epoch 3/15\n",
      "363860/363860 [==============================] - 314s 864us/step - loss: 0.2289 - val_loss: 0.2214\n",
      "Epoch 4/15\n",
      "363860/363860 [==============================] - 314s 863us/step - loss: 0.2197 - val_loss: 0.2141\n",
      "Epoch 5/15\n",
      "363860/363860 [==============================] - 314s 862us/step - loss: 0.2130 - val_loss: 0.2091\n",
      "Epoch 6/15\n",
      "363860/363860 [==============================] - 298s 819us/step - loss: 0.2064 - val_loss: 0.2076\n",
      "Epoch 7/15\n",
      "363860/363860 [==============================] - 280s 768us/step - loss: 0.2022 - val_loss: 0.2061\n",
      "Epoch 8/15\n",
      "363860/363860 [==============================] - 277s 762us/step - loss: 0.1983 - val_loss: 0.2084\n",
      "Epoch 9/15\n",
      "363860/363860 [==============================] - 291s 799us/step - loss: 0.1940 - val_loss: 0.2040\n",
      "Epoch 10/15\n",
      "363860/363860 [==============================] - 316s 868us/step - loss: 0.1898 - val_loss: 0.2026\n",
      "Epoch 11/15\n",
      "363860/363860 [==============================] - 312s 857us/step - loss: 0.1871 - val_loss: 0.2026\n",
      "Epoch 12/15\n",
      "363860/363860 [==============================] - 313s 861us/step - loss: 0.1838 - val_loss: 0.2062\n",
      "Epoch 13/15\n",
      "363860/363860 [==============================] - 316s 868us/step - loss: 0.1815 - val_loss: 0.2043\n",
      "Epoch 14/15\n",
      "363860/363860 [==============================] - 314s 863us/step - loss: 0.1790 - val_loss: 0.2027\n",
      "Epoch 15/15\n",
      "363860/363860 [==============================] - 278s 764us/step - loss: 0.1763 - val_loss: 0.2009\n",
      "0 validation loss: 0.20093529353910172\n",
      "2345796/2345796 [==============================] - 822s 350us/step\n",
      "MODEL: 1\n",
      "Train on 363860 samples, validate on 40430 samples\n",
      "Epoch 1/15\n",
      "363860/363860 [==============================] - 290s 796us/step - loss: 0.2768 - val_loss: 0.2459\n",
      "Epoch 2/15\n",
      "363860/363860 [==============================] - 302s 829us/step - loss: 0.2413 - val_loss: 0.2321\n",
      "Epoch 3/15\n",
      "363860/363860 [==============================] - 342s 940us/step - loss: 0.2278 - val_loss: 0.2192\n",
      "Epoch 4/15\n",
      "363860/363860 [==============================] - 358s 984us/step - loss: 0.2187 - val_loss: 0.2155\n",
      "Epoch 5/15\n",
      "363860/363860 [==============================] - 358s 985us/step - loss: 0.2118 - val_loss: 0.2117\n",
      "Epoch 6/15\n",
      "363860/363860 [==============================] - 346s 952us/step - loss: 0.2055 - val_loss: 0.2138\n",
      "Epoch 7/15\n",
      "363860/363860 [==============================] - 328s 900us/step - loss: 0.2011 - val_loss: 0.2143\n",
      "Epoch 8/15\n",
      "363860/363860 [==============================] - 277s 761us/step - loss: 0.1964 - val_loss: 0.2093\n",
      "Epoch 9/15\n",
      "363860/363860 [==============================] - 309s 849us/step - loss: 0.1927 - val_loss: 0.2066\n",
      "Epoch 10/15\n",
      "363860/363860 [==============================] - 323s 886us/step - loss: 0.1888 - val_loss: 0.2058\n",
      "Epoch 11/15\n",
      "363860/363860 [==============================] - 357s 982us/step - loss: 0.1862 - val_loss: 0.2088\n",
      "Epoch 12/15\n",
      "363860/363860 [==============================] - 322s 885us/step - loss: 0.1827 - val_loss: 0.2084\n",
      "Epoch 13/15\n",
      "363860/363860 [==============================] - 321s 882us/step - loss: 0.1803 - val_loss: 0.2062\n",
      "Epoch 14/15\n",
      "363860/363860 [==============================] - 329s 904us/step - loss: 0.1786 - val_loss: 0.2085\n",
      "Epoch 15/15\n",
      "363860/363860 [==============================] - 316s 868us/step - loss: 0.1751 - val_loss: 0.2108\n",
      "1 validation loss: 0.20582071949748335\n",
      "2345796/2345796 [==============================] - 1081s 461us/step\n",
      "MODEL: 2\n",
      "Train on 363860 samples, validate on 40430 samples\n",
      "Epoch 1/15\n",
      "363860/363860 [==============================] - 455s 1ms/step - loss: 0.2791 - val_loss: 0.2423\n",
      "Epoch 2/15\n",
      "363860/363860 [==============================] - 443s 1ms/step - loss: 0.2419 - val_loss: 0.2274\n",
      "Epoch 3/15\n",
      "363860/363860 [==============================] - 438s 1ms/step - loss: 0.2290 - val_loss: 0.2195\n",
      "Epoch 4/15\n",
      "363860/363860 [==============================] - 443s 1ms/step - loss: 0.2200 - val_loss: 0.2147\n",
      "Epoch 5/15\n",
      "363860/363860 [==============================] - 444s 1ms/step - loss: 0.2129 - val_loss: 0.2126\n",
      "Epoch 6/15\n",
      "363860/363860 [==============================] - 442s 1ms/step - loss: 0.2065 - val_loss: 0.2104\n",
      "Epoch 7/15\n",
      "363860/363860 [==============================] - 460s 1ms/step - loss: 0.2016 - val_loss: 0.2068\n",
      "Epoch 8/15\n",
      "363860/363860 [==============================] - 550s 2ms/step - loss: 0.1976 - val_loss: 0.2078\n",
      "Epoch 9/15\n",
      "363860/363860 [==============================] - 425s 1ms/step - loss: 0.1932 - val_loss: 0.2069\n",
      "Epoch 10/15\n",
      "363860/363860 [==============================] - 324s 890us/step - loss: 0.1895 - val_loss: 0.2054\n",
      "Epoch 11/15\n",
      "363860/363860 [==============================] - 330s 908us/step - loss: 0.1863 - val_loss: 0.2067\n",
      "Epoch 12/15\n",
      "363860/363860 [==============================] - 334s 917us/step - loss: 0.1832 - val_loss: 0.2049\n",
      "Epoch 13/15\n",
      "363860/363860 [==============================] - 336s 923us/step - loss: 0.1808 - val_loss: 0.2044\n",
      "Epoch 14/15\n",
      "363860/363860 [==============================] - 339s 933us/step - loss: 0.1772 - val_loss: 0.2061\n",
      "Epoch 15/15\n",
      "363860/363860 [==============================] - 335s 920us/step - loss: 0.1752 - val_loss: 0.2078\n",
      "2 validation loss: 0.2043754126956756\n",
      "2345796/2345796 [==============================] - 949s 405us/step\n",
      "MODEL: 3\n",
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/15\n",
      "363861/363861 [==============================] - 360s 989us/step - loss: 0.2771 - val_loss: 0.2426\n",
      "Epoch 2/15\n",
      "363861/363861 [==============================] - 361s 992us/step - loss: 0.2407 - val_loss: 0.2280\n",
      "Epoch 3/15\n",
      "363861/363861 [==============================] - 356s 977us/step - loss: 0.2278 - val_loss: 0.2232\n",
      "Epoch 4/15\n",
      "363861/363861 [==============================] - 359s 988us/step - loss: 0.2189 - val_loss: 0.2188\n",
      "Epoch 5/15\n",
      "363861/363861 [==============================] - 358s 984us/step - loss: 0.2121 - val_loss: 0.2182\n",
      "Epoch 6/15\n",
      "363861/363861 [==============================] - 358s 984us/step - loss: 0.2064 - val_loss: 0.2143\n",
      "Epoch 7/15\n",
      "363861/363861 [==============================] - 358s 985us/step - loss: 0.2014 - val_loss: 0.2150\n",
      "Epoch 8/15\n",
      "363861/363861 [==============================] - 359s 988us/step - loss: 0.1972 - val_loss: 0.2106\n",
      "Epoch 9/15\n",
      "363861/363861 [==============================] - 359s 986us/step - loss: 0.1936 - val_loss: 0.2092\n",
      "Epoch 10/15\n",
      "363861/363861 [==============================] - 360s 989us/step - loss: 0.1892 - val_loss: 0.2094\n",
      "Epoch 11/15\n",
      "363861/363861 [==============================] - 359s 986us/step - loss: 0.1865 - val_loss: 0.2101\n",
      "Epoch 12/15\n",
      "363861/363861 [==============================] - 359s 988us/step - loss: 0.1830 - val_loss: 0.2117\n",
      "Epoch 13/15\n",
      "363861/363861 [==============================] - 358s 983us/step - loss: 0.1804 - val_loss: 0.2108\n",
      "Epoch 14/15\n",
      "363861/363861 [==============================] - 360s 989us/step - loss: 0.1780 - val_loss: 0.2107\n",
      "3 validation loss: 0.20918141653050698\n",
      "2345796/2345796 [==============================] - 994s 424us/step\n",
      "MODEL: 4\n",
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363861/363861 [==============================] - 397s 1ms/step - loss: 0.2816 - val_loss: 0.2382\n",
      "Epoch 2/15\n",
      "363861/363861 [==============================] - 396s 1ms/step - loss: 0.2423 - val_loss: 0.2259\n",
      "Epoch 3/15\n",
      "363861/363861 [==============================] - 411s 1ms/step - loss: 0.2295 - val_loss: 0.2149\n",
      "Epoch 4/15\n",
      "363861/363861 [==============================] - 419s 1ms/step - loss: 0.2205 - val_loss: 0.2117\n",
      "Epoch 5/15\n",
      "363861/363861 [==============================] - 395s 1ms/step - loss: 0.2133 - val_loss: 0.2094\n",
      "Epoch 6/15\n",
      "363861/363861 [==============================] - 397s 1ms/step - loss: 0.2078 - val_loss: 0.2078\n",
      "Epoch 7/15\n",
      "363861/363861 [==============================] - 397s 1ms/step - loss: 0.2024 - val_loss: 0.2040\n",
      "Epoch 8/15\n",
      "363861/363861 [==============================] - 394s 1ms/step - loss: 0.1983 - val_loss: 0.2025\n",
      "Epoch 9/15\n",
      "363861/363861 [==============================] - 395s 1ms/step - loss: 0.1939 - val_loss: 0.2043\n",
      "Epoch 10/15\n",
      "363861/363861 [==============================] - 403s 1ms/step - loss: 0.1903 - val_loss: 0.2023\n",
      "Epoch 11/15\n",
      "363861/363861 [==============================] - 395s 1ms/step - loss: 0.1875 - val_loss: 0.2017\n",
      "Epoch 12/15\n",
      "363861/363861 [==============================] - 18900s 52ms/step - loss: 0.1840 - val_loss: 0.2013\n",
      "Epoch 13/15\n",
      "363861/363861 [==============================] - 344s 945us/step - loss: 0.1811 - val_loss: 0.2010\n",
      "Epoch 14/15\n",
      "363861/363861 [==============================] - 348s 957us/step - loss: 0.1791 - val_loss: 0.2019\n",
      "Epoch 15/15\n",
      "363861/363861 [==============================] - 603s 2ms/step - loss: 0.1768 - val_loss: 0.2034\n",
      "4 validation loss: 0.20096851821333953\n",
      "2345796/2345796 [==============================] - 1378s 587us/step\n",
      "MODEL: 5\n",
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/15\n",
      "363861/363861 [==============================] - 640s 2ms/step - loss: 0.2813 - val_loss: 0.2448\n",
      "Epoch 2/15\n",
      "363861/363861 [==============================] - 658s 2ms/step - loss: 0.2422 - val_loss: 0.2295\n",
      "Epoch 3/15\n",
      "363861/363861 [==============================] - 689s 2ms/step - loss: 0.2286 - val_loss: 0.2222\n",
      "Epoch 4/15\n",
      "363861/363861 [==============================] - 729s 2ms/step - loss: 0.2191 - val_loss: 0.2173\n",
      "Epoch 5/15\n",
      "363861/363861 [==============================] - 707s 2ms/step - loss: 0.2120 - val_loss: 0.2134\n",
      "Epoch 6/15\n",
      "363861/363861 [==============================] - 686s 2ms/step - loss: 0.2063 - val_loss: 0.2118\n",
      "Epoch 7/15\n",
      "363861/363861 [==============================] - 687s 2ms/step - loss: 0.2015 - val_loss: 0.2097\n",
      "Epoch 8/15\n",
      "363861/363861 [==============================] - 578s 2ms/step - loss: 0.1967 - val_loss: 0.2092\n",
      "Epoch 9/15\n",
      "363861/363861 [==============================] - 479s 1ms/step - loss: 0.1924 - val_loss: 0.2080\n",
      "Epoch 10/15\n",
      "363861/363861 [==============================] - 415s 1ms/step - loss: 0.1886 - val_loss: 0.2083\n",
      "Epoch 11/15\n",
      "363861/363861 [==============================] - 439s 1ms/step - loss: 0.1859 - val_loss: 0.2092\n",
      "Epoch 12/15\n",
      "363861/363861 [==============================] - 437s 1ms/step - loss: 0.1824 - val_loss: 0.2151\n",
      "Epoch 13/15\n",
      "363861/363861 [==============================] - 438s 1ms/step - loss: 0.1797 - val_loss: 0.2096\n",
      "Epoch 14/15\n",
      "363861/363861 [==============================] - 415s 1ms/step - loss: 0.1773 - val_loss: 0.2090\n",
      "5 validation loss: 0.20796238312136484\n",
      "2345796/2345796 [==============================] - 1100s 469us/step\n",
      "MODEL: 6\n",
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/15\n",
      "363861/363861 [==============================] - 464s 1ms/step - loss: 0.2781 - val_loss: 0.2406\n",
      "Epoch 2/15\n",
      "363861/363861 [==============================] - 501s 1ms/step - loss: 0.2422 - val_loss: 0.2225\n",
      "Epoch 3/15\n",
      "363861/363861 [==============================] - 503s 1ms/step - loss: 0.2286 - val_loss: 0.2156\n",
      "Epoch 4/15\n",
      "363861/363861 [==============================] - 502s 1ms/step - loss: 0.2197 - val_loss: 0.2143\n",
      "Epoch 5/15\n",
      "363861/363861 [==============================] - 503s 1ms/step - loss: 0.2129 - val_loss: 0.2078\n",
      "Epoch 6/15\n",
      "363861/363861 [==============================] - 497s 1ms/step - loss: 0.2074 - val_loss: 0.2057\n",
      "Epoch 7/15\n",
      "363861/363861 [==============================] - 479s 1ms/step - loss: 0.2023 - val_loss: 0.2050\n",
      "Epoch 8/15\n",
      "363861/363861 [==============================] - 490s 1ms/step - loss: 0.1975 - val_loss: 0.2022\n",
      "Epoch 9/15\n",
      "363861/363861 [==============================] - 460s 1ms/step - loss: 0.1931 - val_loss: 0.2023\n",
      "Epoch 10/15\n",
      "363861/363861 [==============================] - 466s 1ms/step - loss: 0.1898 - val_loss: 0.2010\n",
      "Epoch 11/15\n",
      "363861/363861 [==============================] - 479s 1ms/step - loss: 0.1867 - val_loss: 0.2010\n",
      "Epoch 12/15\n",
      "363861/363861 [==============================] - 448s 1ms/step - loss: 0.1840 - val_loss: 0.2005\n",
      "Epoch 13/15\n",
      "363861/363861 [==============================] - 485s 1ms/step - loss: 0.1812 - val_loss: 0.2010\n",
      "Epoch 14/15\n",
      "363861/363861 [==============================] - 472s 1ms/step - loss: 0.1790 - val_loss: 0.2010\n",
      "Epoch 15/15\n",
      "363861/363861 [==============================] - 451s 1ms/step - loss: 0.1759 - val_loss: 0.2037\n",
      "6 validation loss: 0.20046410990419186\n",
      "2345796/2345796 [==============================] - 1028s 438us/step\n",
      "MODEL: 7\n",
      "Train on 363862 samples, validate on 40428 samples\n",
      "Epoch 1/15\n",
      "363862/363862 [==============================] - 464s 1ms/step - loss: 0.2778 - val_loss: 0.2451\n",
      "Epoch 2/15\n",
      "363862/363862 [==============================] - 663s 2ms/step - loss: 0.2409 - val_loss: 0.2272\n",
      "Epoch 3/15\n",
      "363862/363862 [==============================] - 929s 3ms/step - loss: 0.2280 - val_loss: 0.2232\n",
      "Epoch 4/15\n",
      "363862/363862 [==============================] - 940s 3ms/step - loss: 0.2188 - val_loss: 0.2153\n",
      "Epoch 5/15\n",
      "363862/363862 [==============================] - 870s 2ms/step - loss: 0.2119 - val_loss: 0.2135\n",
      "Epoch 6/15\n",
      "363862/363862 [==============================] - 905s 2ms/step - loss: 0.2064 - val_loss: 0.2203\n",
      "Epoch 7/15\n",
      "363862/363862 [==============================] - 968s 3ms/step - loss: 0.2014 - val_loss: 0.2097\n",
      "Epoch 8/15\n",
      "363862/363862 [==============================] - 922s 3ms/step - loss: 0.1971 - val_loss: 0.2106\n",
      "Epoch 9/15\n",
      "363862/363862 [==============================] - 932s 3ms/step - loss: 0.1930 - val_loss: 0.2061\n",
      "Epoch 10/15\n",
      "363862/363862 [==============================] - 902s 2ms/step - loss: 0.1892 - val_loss: 0.2052\n",
      "Epoch 11/15\n",
      "363862/363862 [==============================] - 782s 2ms/step - loss: 0.1866 - val_loss: 0.2051\n",
      "Epoch 12/15\n",
      "363862/363862 [==============================] - 549s 2ms/step - loss: 0.1831 - val_loss: 0.2093\n",
      "Epoch 13/15\n",
      "363862/363862 [==============================] - 545s 1ms/step - loss: 0.1801 - val_loss: 0.2093\n",
      "Epoch 14/15\n",
      "363862/363862 [==============================] - 534s 1ms/step - loss: 0.1782 - val_loss: 0.2085\n",
      "Epoch 15/15\n",
      "363862/363862 [==============================] - 530s 1ms/step - loss: 0.1760 - val_loss: 0.2092\n",
      "7 validation loss: 0.20509785396160554\n",
      "2345796/2345796 [==============================] - 1187s 506us/step\n",
      "MODEL: 8\n",
      "Train on 363862 samples, validate on 40428 samples\n",
      "Epoch 1/15\n",
      "363862/363862 [==============================] - 542s 1ms/step - loss: 0.2785 - val_loss: 0.2385\n",
      "Epoch 2/15\n",
      "363862/363862 [==============================] - 561s 2ms/step - loss: 0.2418 - val_loss: 0.2235\n",
      "Epoch 3/15\n",
      "363862/363862 [==============================] - 556s 2ms/step - loss: 0.2280 - val_loss: 0.2155\n",
      "Epoch 4/15\n",
      "363862/363862 [==============================] - 561s 2ms/step - loss: 0.2194 - val_loss: 0.2121\n",
      "Epoch 5/15\n",
      "363862/363862 [==============================] - 547s 2ms/step - loss: 0.2122 - val_loss: 0.2088\n",
      "Epoch 6/15\n",
      "363862/363862 [==============================] - 544s 1ms/step - loss: 0.2066 - val_loss: 0.2053\n",
      "Epoch 7/15\n",
      "363862/363862 [==============================] - 583s 2ms/step - loss: 0.2018 - val_loss: 0.2044\n",
      "Epoch 8/15\n",
      "363862/363862 [==============================] - 963s 3ms/step - loss: 0.1969 - val_loss: 0.2035\n",
      "Epoch 9/15\n",
      "363862/363862 [==============================] - 952s 3ms/step - loss: 0.1937 - val_loss: 0.2027\n",
      "Epoch 10/15\n",
      "363862/363862 [==============================] - 949s 3ms/step - loss: 0.1900 - val_loss: 0.2025\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363862/363862 [==============================] - 529s 1ms/step - loss: 0.1871 - val_loss: 0.2029\n",
      "Epoch 12/15\n",
      "363862/363862 [==============================] - 488s 1ms/step - loss: 0.1837 - val_loss: 0.2023\n",
      "Epoch 13/15\n",
      "363862/363862 [==============================] - 491s 1ms/step - loss: 0.1813 - val_loss: 0.2024\n",
      "Epoch 14/15\n",
      "363862/363862 [==============================] - 492s 1ms/step - loss: 0.1786 - val_loss: 0.2020\n",
      "Epoch 15/15\n",
      "363862/363862 [==============================] - 488s 1ms/step - loss: 0.1762 - val_loss: 0.2030\n",
      "8 validation loss: 0.2019525884006637\n",
      "2345796/2345796 [==============================] - 1278s 545us/step\n",
      "MODEL: 9\n",
      "Train on 363862 samples, validate on 40428 samples\n",
      "Epoch 1/15\n",
      "363862/363862 [==============================] - 1049s 3ms/step - loss: 0.2772 - val_loss: 0.2482\n",
      "Epoch 2/15\n",
      "363862/363862 [==============================] - 1073s 3ms/step - loss: 0.2416 - val_loss: 0.2280\n",
      "Epoch 3/15\n",
      "363862/363862 [==============================] - 1074s 3ms/step - loss: 0.2278 - val_loss: 0.2212\n",
      "Epoch 4/15\n",
      "363862/363862 [==============================] - 1065s 3ms/step - loss: 0.2198 - val_loss: 0.2148\n",
      "Epoch 5/15\n",
      "363862/363862 [==============================] - 1073s 3ms/step - loss: 0.2125 - val_loss: 0.2118\n",
      "Epoch 6/15\n",
      "363862/363862 [==============================] - 1076s 3ms/step - loss: 0.2067 - val_loss: 0.2090\n",
      "Epoch 7/15\n",
      "363862/363862 [==============================] - 1075s 3ms/step - loss: 0.2020 - val_loss: 0.2087\n",
      "Epoch 8/15\n",
      "363862/363862 [==============================] - 1358s 4ms/step - loss: 0.1965 - val_loss: 0.2053\n",
      "Epoch 9/15\n",
      "363862/363862 [==============================] - 1134s 3ms/step - loss: 0.1933 - val_loss: 0.2077\n",
      "Epoch 10/15\n",
      "363862/363862 [==============================] - 683s 2ms/step - loss: 0.1898 - val_loss: 0.2073\n",
      "Epoch 11/15\n",
      "363862/363862 [==============================] - 640s 2ms/step - loss: 0.1868 - val_loss: 0.2050\n",
      "Epoch 12/15\n",
      "363862/363862 [==============================] - 606s 2ms/step - loss: 0.1830 - val_loss: 0.2042\n",
      "Epoch 13/15\n",
      "363862/363862 [==============================] - 605s 2ms/step - loss: 0.1801 - val_loss: 0.2078\n",
      "Epoch 14/15\n",
      "363862/363862 [==============================] - 655s 2ms/step - loss: 0.1783 - val_loss: 0.2103\n",
      "Epoch 15/15\n",
      "363862/363862 [==============================] - 608s 2ms/step - loss: 0.1760 - val_loss: 0.2073\n",
      "9 validation loss: 0.20417431928567567\n",
      "2345796/2345796 [==============================] - 1348s 575us/step\n"
     ]
    }
   ],
   "source": [
    "for idx_train, idx_val in skf.split(train[\"is_duplicate\"], train[\"is_duplicate\"]):\n",
    "    print(\"MODEL:\", model_count)\n",
    "    data_1_train = data_1[idx_train]\n",
    "    data_2_train = data_2[idx_train]\n",
    "    labels_train = labels[idx_train]\n",
    "    f_train = features_train[idx_train]\n",
    "\n",
    "    data_1_val = data_1[idx_val]\n",
    "    data_2_val = data_2[idx_val]\n",
    "    labels_val = labels[idx_val]\n",
    "    f_val = features_train[idx_val]\n",
    "\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    features_input = Input(shape=(f_train.shape[1],), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.2)(features_dense)\n",
    "\n",
    "    addition = add([x1, y1])\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"nadam\")\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    best_model_path = \"best_model\" + str(model_count) + \".h5\"\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    hist = model.fit([data_1_train, data_2_train, f_train], labels_train,\n",
    "                     validation_data=([data_1_val, data_2_val, f_val], labels_val),\n",
    "                     epochs=15, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                     callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "    model.load_weights(best_model_path)\n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "\n",
    "    preds = model.predict([test_data_1, test_data_2, features_test], batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    submission = pd.DataFrame({\"test_id\": test[\"test_id\"], \"is_duplicate\": preds.ravel()})\n",
    "    submission.to_csv(\"preds\" + str(model_count) + \".csv\", index=False)\n",
    "\n",
    "    model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
